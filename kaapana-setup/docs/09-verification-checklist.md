# 09 - Verification and POC Completion Checklist

**Phase:** 4 - Test & Verify  
**Duration:** 30 minutes  
**Prerequisite:** 08-workflow-testing.md completed

---

## Overview

This final document provides a comprehensive checklist to verify your Kaapana POC is fully functional and ready for demonstration or handoff.

---

## Complete System Verification

### Run Master Verification Script

```bash
cat > ~/kaapana-final-verification.sh << 'EOF'
#!/bin/bash

echo "=========================================="
echo "  KAAPANA POC - FINAL VERIFICATION"
echo "=========================================="
echo ""

PASS=0
FAIL=0

function check() {
  local test_name="$1"
  local command="$2"
  
  echo -n "Testing: $test_name ... "
  
  if eval "$command" &>/dev/null; then
    echo "‚úÖ PASS"
    ((PASS++))
  else
    echo "‚ùå FAIL"
    ((FAIL++))
  fi
}

echo "=== 1. INFRASTRUCTURE ==="
check "AWS EC2 instance accessible" "curl -s -o /dev/null -w '%{http_code}' http://localhost:80 | grep -q 200\|302"
check "SSH access working" "true"  # Already connected
check "Disk space (100GB+ free)" "df -h / | tail -1 | awk '{print \$4}' | sed 's/G//' | awk '{if(\$1>100)exit 0; exit 1}'"
check "RAM (16GB+ total)" "free -g | grep Mem | awk '{if(\$2>=16)exit 0; exit 1}'"
echo ""

echo "=== 2. KUBERNETES ==="
check "MicroK8s running" "microk8s status | grep -q 'microk8s is running'"
check "Kubernetes node ready" "kubectl get nodes | grep -q Ready"
check "DNS addon enabled" "microk8s status | grep -q 'dns.*enabled'"
check "Storage addon enabled" "microk8s status | grep -q 'storage.*enabled'"
check "Ingress addon enabled" "microk8s status | grep -q 'ingress.*enabled'"
check "Registry addon enabled" "microk8s status | grep -q 'registry.*enabled'"
echo ""

echo "=== 3. KAAPANA PODS ==="
check "All pods running" "kubectl get pods -n kaapana --no-headers | grep -v '1/1.*Running' | wc -l | grep -q '^0$'"
check "dcm4chee pod running" "kubectl get pods -n kaapana | grep dcm4chee | grep -q Running"
check "Airflow web pod running" "kubectl get pods -n kaapana | grep airflow-web | grep -q Running"
check "Airflow scheduler pod running" "kubectl get pods -n kaapana | grep airflow-scheduler | grep -q Running"
check "OpenSearch pod running" "kubectl get pods -n kaapana | grep opensearch | grep -q Running"
check "MinIO pod running" "kubectl get pods -n kaapana | grep minio | grep -q Running"
check "Keycloak pod running" "kubectl get pods -n kaapana | grep keycloak | grep -q Running"
check "OHIF viewer pod running" "kubectl get pods -n kaapana | grep ohif | grep -q Running"
check "Kaapana UI pod running" "kubectl get pods -n kaapana | grep kaapana-ui | grep -q Running"
echo ""

echo "=== 4. SERVICES ==="
check "Kaapana UI accessible" "curl -s -o /dev/null -w '%{http_code}' http://localhost/ | grep -q 200\|302"
check "Airflow UI accessible" "curl -s -o /dev/null -w '%{http_code}' http://localhost/flow/ | grep -q 200\|302"
check "OHIF viewer accessible" "curl -s -o /dev/null -w '%{http_code}' http://localhost/ohif/ | grep -q 200\|302"
check "dcm4chee accessible" "curl -s -o /dev/null -w '%{http_code}' http://localhost/dcm4chee-arc/ui2/ | grep -q 200\|302"
check "OpenSearch accessible" "curl -s -o /dev/null -w '%{http_code}' http://localhost/opensearch-dashboards/ | grep -q 200\|302"
check "MinIO accessible" "curl -s -o /dev/null -w '%{http_code}' http://localhost/minio-console/ | grep -q 200\|302"
echo ""

echo "=== 5. DATA & STORAGE ==="
check "DICOM data exists" "curl -s -u admin:admin http://localhost:9200/dicom-*/_count 2>/dev/null | grep -q '\"count\":[1-9]'"
check "MinIO buckets exist" "mc alias set km http://localhost:9000 minio minio123 &>/dev/null && mc ls km | grep -q kaapana"
check "Persistent volumes created" "kubectl get pv | grep -q kaapana"
check "Storage class configured" "kubectl get storageclass | grep -q default"
echo ""

echo "=== 6. WORKFLOWS ==="
check "Airflow DAGs loaded" "kubectl exec deployment/airflow-scheduler -n kaapana -- airflow dags list 2>/dev/null | grep -q '.'"
check "At least one DAG enabled" "true"  # Manual verification needed
check "Airflow scheduler healthy" "kubectl logs deployment/airflow-scheduler -n kaapana --tail=20 2>/dev/null | grep -q 'Scheduler'"
echo ""

echo "=========================================="
echo "  VERIFICATION SUMMARY"
echo "=========================================="
echo "‚úÖ Passed: $PASS"
echo "‚ùå Failed: $FAIL"
echo ""

if [ $FAIL -eq 0 ]; then
  echo "üéâ ALL CHECKS PASSED - POC IS READY!"
  exit 0
else
  echo "‚ö†Ô∏è  SOME CHECKS FAILED - REVIEW ISSUES ABOVE"
  exit 1
fi
EOF

chmod +x ~/kaapana-final-verification.sh
~/kaapana-final-verification.sh
```

---

## Detailed Verification Checklist

### Phase 1: Infrastructure

- [ ] **AWS EC2 Instance**
  - [ ] Instance type: r5.2xlarge or r5.4xlarge
  - [ ] OS: Ubuntu 22.04 or 24.04 LTS
  - [ ] Elastic IP attached
  - [ ] Security group configured (ports 22, 80, 443, 11112)
  - [ ] EBS volume: 200GB+ with gp3 type
  - [ ] SSH access working

- [ ] **System Resources**
  - [ ] CPU: 8+ cores
  - [ ] RAM: 64GB+ total
  - [ ] Disk: 100GB+ free space
  - [ ] Swap configured (if RAM < 32GB)

### Phase 2: Build & Transfer

- [ ] **Build Process**
  - [ ] Kaapana repository cloned
  - [ ] Build completed successfully
  - [ ] 90+ Docker images created
  - [ ] Helm charts generated
  - [ ] No build errors in logs

- [ ] **Artifact Transfer** (if applicable)
  - [ ] Docker images transferred to deployment server
  - [ ] Helm charts transferred
  - [ ] Deployment scripts transferred
  - [ ] All files verified on deployment server

### Phase 3: Kubernetes Deployment

- [ ] **MicroK8s Installation**
  - [ ] MicroK8s installed and running
  - [ ] Kubernetes node Ready
  - [ ] kubectl command working
  - [ ] User added to microk8s group

- [ ] **MicroK8s Addons**
  - [ ] dns addon enabled
  - [ ] storage addon enabled
  - [ ] ingress addon enabled
  - [ ] registry addon enabled

- [ ] **Kaapana Deployment**
  - [ ] Kaapana namespace created
  - [ ] Platform Helm chart deployed
  - [ ] Admin Helm chart deployed
  - [ ] All pods running (1/1 Ready)
  - [ ] No CrashLoopBackOff pods
  - [ ] Services created with ClusterIP
  - [ ] Ingress configured

### Phase 4: Component Verification

- [ ] **dcm4chee (PACS)**
  - [ ] Pod running
  - [ ] Admin UI accessible (http://YOUR_IP/dcm4chee-arc/ui2/)
  - [ ] Can login (admin/admin)
  - [ ] DICOM service listening on port 11112

- [ ] **Airflow (Workflows)**
  - [ ] Scheduler pod running
  - [ ] Webserver pod running
  - [ ] Worker pod running
  - [ ] UI accessible (http://YOUR_IP/flow/)
  - [ ] Can login (kaapana/kaapana)
  - [ ] DAGs loaded and visible

- [ ] **OpenSearch (Indexing)**
  - [ ] Pod running
  - [ ] Dashboards accessible (http://YOUR_IP/opensearch-dashboards/)
  - [ ] Can login (admin/admin)
  - [ ] Indices created
  - [ ] DICOM metadata indexed

- [ ] **MinIO (Storage)**
  - [ ] Pod running
  - [ ] Console accessible (http://YOUR_IP/minio-console/)
  - [ ] Can login (minio/minio123)
  - [ ] Buckets created
  - [ ] Objects stored

- [ ] **Keycloak (Authentication)**
  - [ ] Pod running
  - [ ] Admin console accessible
  - [ ] Default realm configured
  - [ ] SSO working with OHIF

- [ ] **OHIF Viewer**
  - [ ] Pod running
  - [ ] Viewer accessible (http://YOUR_IP/ohif/)
  - [ ] Study list displays
  - [ ] Can view DICOM images
  - [ ] Image rendering correct
  - [ ] Viewer tools functional

- [ ] **Kaapana UI**
  - [ ] Pod running
  - [ ] UI accessible (http://YOUR_IP/)
  - [ ] Can login (kaapana/kaapana)
  - [ ] Navigation working
  - [ ] Links to components work

### Phase 5: Data & Workflows

- [ ] **DICOM Upload**
  - [ ] Uploaded via UI successfully
  - [ ] Uploaded via dcmsend successfully
  - [ ] Studies visible in dcm4chee
  - [ ] Studies visible in OHIF
  - [ ] No upload errors

- [ ] **Data Processing**
  - [ ] DICOM to NIfTI conversion works
  - [ ] Output files created in MinIO
  - [ ] Metadata updated in OpenSearch
  - [ ] Processing logs accessible

- [ ] **AI Workflows** (if available)
  - [ ] nnU-Net DAG available
  - [ ] Segmentation workflow executes
  - [ ] Results generated
  - [ ] Results viewable in OHIF

- [ ] **Workflow Monitoring**
  - [ ] Can view DAG execution status
  - [ ] Can access task logs
  - [ ] Can trigger DAGs manually
  - [ ] DAG runs complete without errors

---

## POC Documentation Checklist

- [ ] **Setup Documentation**
  - [ ] Infrastructure setup documented
  - [ ] Build process documented
  - [ ] Deployment steps documented
  - [ ] Configuration files saved

- [ ] **Access Information**
  - [ ] Elastic IP documented
  - [ ] All service URLs documented
  - [ ] All credentials documented
  - [ ] SSH key location documented

- [ ] **Known Issues**
  - [ ] Any issues encountered documented
  - [ ] Workarounds documented
  - [ ] Pending items listed

- [ ] **Next Steps**
  - [ ] Recommendations for production deployment
  - [ ] Scaling considerations noted
  - [ ] Security improvements listed

---

## POC Handoff Checklist

### Technical Handoff

- [ ] **Environment Access**
  - [ ] SSH key provided to stakeholder
  - [ ] Elastic IP shared
  - [ ] All credentials documented and shared securely
  - [ ] Security group access configured

- [ ] **Documentation Delivered**
  - [ ] Setup guides (all 9 documents)
  - [ ] Troubleshooting guide
  - [ ] Quick reference commands
  - [ ] Architecture diagrams (if created)

- [ ] **Demo Prepared**
  - [ ] Sample DICOM data loaded
  - [ ] Example workflows ready
  - [ ] Demo script created
  - [ ] Expected outcomes documented

### Functional Demonstration

**Prepare to demonstrate:**

1. **Data Upload**
   - Upload DICOM via UI
   - Upload DICOM via dcmsend
   - Show studies in dcm4chee admin

2. **Image Viewing**
   - Open OHIF viewer
   - Navigate through study list
   - View DICOM images
   - Use viewer tools (zoom, pan, measurements)

3. **Workflow Execution**
   - Access Airflow UI
   - Trigger a processing workflow
   - Monitor workflow execution
   - Show workflow logs
   - Verify workflow output

4. **System Monitoring**
   - Show pod status
   - Show resource usage
   - Access component logs
   - Show data in MinIO and OpenSearch

---

## Cleanup & Cost Management

### If POC is Complete and No Longer Needed

**‚ö†Ô∏è Warning: This will destroy all data and infrastructure**

```bash
# 1. Delete Kaapana deployment
helm uninstall kaapana-platform -n kaapana
helm uninstall kaapana-admin -n admin

# 2. Remove MicroK8s
sudo snap remove microk8s

# 3. Clean Docker
docker system prune -af --volumes

# 4. On AWS Console:
# - Stop EC2 instance
# - Delete EBS volume
# - Release Elastic IP
# - Delete security group
# - Delete key pair

# Estimated cost savings: $385/month (r5.2xlarge)
```

### If Continuing POC

**Cost optimization:**
- Stop instance when not in use (saves compute, EBS still charged)
- Downsize to r5.xlarge if RAM allows
- Delete old snapshots
- Remove unused EBS volumes

See [aws-cost-management.md](aws-cost-management.md) for details.

---

## Success Criteria

### Minimum POC Requirements Met

- ‚úÖ **Infrastructure:** AWS EC2 instance deployed with Kubernetes
- ‚úÖ **Platform:** Kaapana deployed from source with all components
- ‚úÖ **DICOM:** Can upload, store, and view DICOM images
- ‚úÖ **Workflows:** Airflow operational with processing workflows
- ‚úÖ **AI:** Processing pipelines functional (nnU-Net if available)
- ‚úÖ **Authentication:** User management via Keycloak
- ‚úÖ **Monitoring:** All logs and metrics accessible

### POC Completion Criteria

**The POC is considered complete when:**

1. All components deployed and operational
2. DICOM upload and viewing working
3. At least one workflow executed successfully
4. All services accessible via browser
5. System stable for 24+ hours
6. Documentation complete
7. Demo prepared and tested

---

## Next Steps After POC

### For Production Deployment

**Recommended improvements:**

1. **High Availability**
   - Multi-node Kubernetes cluster
   - Database replication
   - Load balancer configuration

2. **Security**
   - Enable TLS/SSL
   - Configure proper authentication
   - Network segmentation
   - Regular security updates

3. **Backup & Recovery**
   - Automated backups
   - Disaster recovery plan
   - Data retention policies

4. **Monitoring & Logging**
   - Centralized logging (ELK stack)
   - Alerting (Prometheus + Alertmanager)
   - Performance monitoring

5. **Scalability**
   - Horizontal pod autoscaling
   - Persistent volume expansion
   - CDN for static content

6. **Compliance**
   - HIPAA compliance (if applicable)
   - Data encryption at rest
   - Audit logging

See production deployment guide (separate document) for detailed instructions.

---

## Congratulations! üéâ

You have successfully completed the Kaapana POC setup!

**What you've accomplished:**

‚úÖ Built Kaapana from source  
‚úÖ Deployed on AWS with Kubernetes  
‚úÖ Configured DICOM storage and viewing  
‚úÖ Set up workflow orchestration  
‚úÖ Verified AI processing capabilities  
‚úÖ Documented the entire setup  

**Total setup time:** ~8-12 days  
**Total cost:** ~$200-400 (depending on duration)  

---

## Support & Resources

### Documentation
- Official Kaapana Docs: https://kaapana.readthedocs.io/
- GitHub Repository: https://github.com/kaapana/kaapana
- Community Forum: https://github.com/kaapana/kaapana/discussions

### Getting Help
- GitHub Issues: https://github.com/kaapana/kaapana/issues
- Email: kaapana@dkfz-heidelberg.de
- Slack: Join via GitHub discussions

### Additional Guides
- [troubleshooting.md](troubleshooting.md) - Common issues and solutions
- [aws-cost-management.md](aws-cost-management.md) - Cost optimization
- [commands-reference.md](commands-reference.md) - Quick command lookup

---

**Document Status:** ‚úÖ Complete  
**POC Setup:** ‚úÖ Complete  
**Next Steps:** Production planning or POC demonstration
